{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initial Setup\n",
    "Let's begin with imports, as well as environment and figure configuartion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# cleanup if setup is reinitialized\n",
    "import os\n",
    "try:\n",
    "    os.remove('snorkel.db')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "from snorkel import *\n",
    "import extractions\n",
    "import re\n",
    "import cPickle, sys, matplotlib\n",
    "import unicodedata\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "#sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (18,6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1-2) Preprocessing & Candidate (Relation) Extraction Pipeline\n",
    "This process includes the following steps:\n",
    "*  [1] Preprocessing our raw data files (includes tagging).\n",
    "* [2a] Creating \"sub\"-parsers: Document and Sentence varieties. \n",
    "* [2b] Loading our Corpus with a CorpusParser.\n",
    "* [2c] Candidate Extraction.\n",
    "* [2d] Relations Generation (directly below; calls steps 1-4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus made\n",
      "session not none 1; adding corpus to session\n",
      "appending doc to corpus 1 Document 28181487\n",
      "appending doc to corpus 2 Document 28181578\n",
      "appending doc to corpus 3 Document 28181552\n",
      "appending doc to corpus 4 Document 28181589\n",
      "appending doc to corpus 5 Document 28181483\n",
      "appending doc to corpus 6 Document 28182994\n",
      "appending doc to corpus 7 Document 5299079\n",
      "appending doc to corpus 8 Document 5299068\n",
      "appending doc to corpus 9 Document 28176819\n",
      "appending doc to corpus 10 Document 28176872\n",
      "appending doc to corpus 11 Document 28173772\n",
      "appending doc to corpus 12 Document 5297221\n",
      "appending doc to corpus 13 Document 28176833\n",
      "appending doc to corpus 14 Document 28176818\n",
      "appending doc to corpus 15 Document 28176844\n",
      "appending doc to corpus 16 Document 28176788\n",
      "appending doc to corpus 17 Document 28176805\n",
      "appending doc to corpus 18 Document 28176803\n",
      "appending doc to corpus 19 Document 28176880\n",
      "appending doc to corpus 20 Document 28176845\n",
      "appending doc to corpus 21 Document 28174312\n",
      "appending doc to corpus 22 Document 28174310\n",
      "appending doc to corpus 23 Document 5294742\n",
      "appending doc to corpus 24 Document 28167241\n",
      "appending doc to corpus 25 Document 28167243\n",
      "appending doc to corpus 26 Document 5291739\n",
      "appending doc to corpus 27 Document 5291737\n",
      "appending doc to corpus 28 Document 5291735\n",
      "appending doc to corpus 29 Document 5291733\n",
      "appending doc to corpus 30 Document 5291731\n",
      "appending doc to corpus 31 Document 5291729\n",
      "appending doc to corpus 32 Document 5291727\n",
      "appending doc to corpus 33 Document 5291725\n",
      "appending doc to corpus 34 Document 5291319\n",
      "appending doc to corpus 35 Document 28122229\n",
      "appending doc to corpus 36 Document 5286542\n",
      "appending doc to corpus 37 Document 5286538\n",
      "appending doc to corpus 38 Document 5286467\n",
      "appending doc to corpus 39 Document 28000020\n",
      "appending doc to corpus 40 Document 28144859\n",
      "appending doc to corpus 41 Document 26980008\n",
      "appending doc to corpus 42 Document 27783317\n",
      "appending doc to corpus 43 Document 27514643\n",
      "appending doc to corpus 44 Document 5283509\n",
      "appending doc to corpus 45 Document 26902522\n",
      "appending doc to corpus 46 Document 5282457\n",
      "appending doc to corpus 47 Document 5282455\n",
      "appending doc to corpus 48 Document 5282448\n",
      "appending doc to corpus 49 Document 5282446\n",
      "appending doc to corpus 50 Document 5282442\n",
      "appending doc to corpus 51 Document 5282436\n",
      "appending doc to corpus 52 Document 5282434\n",
      "appending doc to corpus 53 Document 5282432\n",
      "appending doc to corpus 54 Document 5282428\n",
      "appending doc to corpus 55 Document 5282420\n",
      "appending doc to corpus 56 Document 5282418\n",
      "appending doc to corpus 57 Document 27838763\n",
      "appending doc to corpus 58 Document 27554774\n",
      "appending doc to corpus 59 Document 27866241\n",
      "appending doc to corpus 60 Document 27379386\n",
      "appending doc to corpus 61 Document 26419597\n",
      "appending doc to corpus 62 Document 5281646\n",
      "appending doc to corpus 63 Document 26649857\n",
      "appending doc to corpus 64 Document 28067629\n",
      "appending doc to corpus 65 Document 27935819\n",
      "appending doc to corpus 66 Document 28067628\n",
      "appending doc to corpus 67 Document 27895052\n",
      "appending doc to corpus 68 Document 27881437\n",
      "appending doc to corpus 69 Document 28089995\n",
      "appending doc to corpus 70 Document 27934662\n",
      "appending doc to corpus 71 Document 27875242\n",
      "appending doc to corpus 72 Document 5278260\n",
      "appending doc to corpus 73 Document 5277010\n",
      "appending doc to corpus 74 Document 5277008\n",
      "appending doc to corpus 75 Document 5276996\n",
      "appending doc to corpus 76 Document 5276855\n",
      "appending doc to corpus 77 Document 5276851\n",
      "appending doc to corpus 78 Document 5276820\n",
      "appending doc to corpus 79 Document 5276816\n",
      "appending doc to corpus 80 Document 5276812\n",
      "appending doc to corpus 81 Document 5276808\n",
      "appending doc to corpus 82 Document 28130689\n",
      "appending doc to corpus 83 Document 5270378\n",
      "appending doc to corpus 84 Document 5270363\n",
      "appending doc to corpus 85 Document 26552421\n",
      "appending doc to corpus 86 Document 27736304\n",
      "appending doc to corpus 87 Document 22660553\n",
      "appending doc to corpus 88 Document 19496923\n",
      "appending doc to corpus 89 Document 5278518\n",
      "appending doc to corpus 90 Document 5278232\n",
      "appending doc to corpus 91 Document 5278230\n",
      "appending doc to corpus 92 Document 5278226\n",
      "appending doc to corpus 93 Document 5278224\n",
      "appending doc to corpus 94 Document 5278222\n",
      "appending doc to corpus 95 Document 5278218\n",
      "appending doc to corpus 96 Document 5278216\n",
      "appending doc to corpus 97 Document 5278212\n",
      "appending doc to corpus 98 Document 5278210\n",
      "appending doc to corpus 99 Document 5278202\n",
      "appending doc to corpus 100 Document 5278198\n",
      "appending doc to corpus 101 Document 5278196\n",
      "appending doc to corpus 102 Document 5278192\n",
      "appending doc to corpus 103 Document 5278190\n",
      "appending doc to corpus 104 Document 5278184\n",
      "appending doc to corpus 105 Document 5305199\n",
      "appending doc to corpus 106 Document 5305183\n",
      "appending doc to corpus 107 Document 5305182\n",
      "appending doc to corpus 108 Document 27562956\n",
      "appending doc to corpus 109 Document 26335740\n",
      "appending doc to corpus 110 Document 5304469\n",
      "appending doc to corpus 111 Document 26778574\n",
      "appending doc to corpus 112 Document 24029061\n",
      "appending doc to corpus 113 Document 24946761\n",
      "appending doc to corpus 114 Document 27418145\n",
      "appending doc to corpus 115 Document 27429198\n",
      "appending doc to corpus 116 Document 27385000\n",
      "appending doc to corpus 117 Document 28087951\n",
      "appending doc to corpus 118 Document 27344180\n",
      "appending doc to corpus 119 Document 27329598\n",
      "appending doc to corpus 120 Document 27528032\n",
      "appending doc to corpus 121 Document 27528026\n",
      "appending doc to corpus 122 Document 27527864\n",
      "appending doc to corpus 123 Document 27494859\n",
      "appending doc to corpus 124 Document 27494851\n",
      "appending doc to corpus 125 Document 27489353\n",
      "appending doc to corpus 126 Document 27486768\n",
      "appending doc to corpus 127 Document 27486972\n",
      "appending doc to corpus 128 Document 27486766\n",
      "appending doc to corpus 129 Document 27486754\n",
      "appending doc to corpus 130 Document 27472395\n",
      "appending doc to corpus 131 Document 27462864\n",
      "appending doc to corpus 132 Document 27462774\n",
      "appending doc to corpus 133 Document 27447560\n",
      "appending doc to corpus 134 Document 27527859\n",
      "appending doc to corpus 135 Document 27447860\n",
      "appending doc to corpus 136 Document 27494855\n",
      "appending doc to corpus 137 Document 27494863\n",
      "appending doc to corpus 138 Document 27489357\n",
      "appending doc to corpus 139 Document 27448961\n",
      "appending doc to corpus 140 Document 27489354\n",
      "appending doc to corpus 141 Document 27486767\n",
      "appending doc to corpus 142 Document 27487147\n",
      "appending doc to corpus 143 Document 27486763\n",
      "appending doc to corpus 144 Document 27487139\n",
      "appending doc to corpus 145 Document 27486820\n",
      "appending doc to corpus 146 Document 27472388\n",
      "appending doc to corpus 147 Document 27409165\n",
      "appending doc to corpus 148 Document 27462779\n",
      "appending doc to corpus 149 Document 27494845\n",
      "appending doc to corpus 150 Document 27191652\n",
      "appending doc to corpus 151 Document 27486823\n",
      "appending doc to corpus 152 Document 27621042\n",
      "appending doc to corpus 153 Document 27487143\n",
      "appending doc to corpus 154 Document 27486753\n",
      "appending doc to corpus 155 Document 27528230\n",
      "appending doc to corpus 156 Document 27463015\n",
      "appending doc to corpus 157 Document 27463011\n",
      "appending doc to corpus 158 Document 27509060\n",
      "appending doc to corpus 159 Document 28067618\n",
      "appending doc to corpus 160 Document 28117666\n",
      "appending doc to corpus 161 Document 28182867\n",
      "appending doc to corpus 162 Document 27528226\n",
      "appending doc to corpus 163 Document 27462861\n",
      "appending doc to corpus 164 Document 27447743\n",
      "appending doc to corpus 165 Document 5301370\n",
      "appending doc to corpus 166 Document 28186128\n",
      "appending doc to corpus 167 Document 28186209\n",
      "appending doc to corpus 168 Document 28186136\n",
      "appending doc to corpus 169 Document 28186195\n",
      "appending doc to corpus 170 Document 28186192\n",
      "appending doc to corpus 171 Document 28186200\n",
      "appending doc to corpus 172 Document 28186176\n",
      "appending doc to corpus 173 Document 28186197\n",
      "appending doc to corpus 174 Document 5301067\n",
      "appending doc to corpus 175 Document 5300914\n",
      "appending doc to corpus 176 Document 5300912\n",
      "appending doc to corpus 177 Document 27771939\n",
      "appending doc to corpus 178 Document 27666942\n",
      "appending doc to corpus 179 Document 27595778\n",
      "appending doc to corpus 180 Document 27530202\n",
      "appending doc to corpus 181 Document 28189067\n",
      "appending doc to corpus 182 Document 5299996\n",
      "appending doc to corpus 183 Document 28181991\n",
      "appending doc to corpus 184 Document 28181555\n",
      "appending doc to corpus 185 Document 28181564\n",
      "appending doc to corpus 186 Document 28181486\n",
      "appending doc to corpus 187 Document 28179014\n",
      "appending doc to corpus 188 Document 28178929\n",
      "appending doc to corpus 189 Document 28178951\n",
      "appending doc to corpus 190 Document 5299773\n",
      "appending doc to corpus 191 Document 28179026\n",
      "appending doc to corpus 192 Document 28179020\n",
      "appending doc to corpus 193 Document 28179007\n",
      "appending doc to corpus 194 Document 28178999\n",
      "appending doc to corpus 195 Document 28183321\n",
      "appending doc to corpus 196 Document 28178969\n",
      "appending doc to corpus 197 Document 28178994\n",
      "appending doc to corpus 198 Document 5299695\n",
      "appending doc to corpus 199 Document 28178938\n",
      "appending doc to corpus 200 Document 5299675\n",
      "session not none 2; committing corpus to session\n",
      "Number of documents: 200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 0, not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-714655111346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"articles/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBiomarkerDiseaseRelations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dump relations just in case iPython breaks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"articles.relations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/Documents/MallickLab/MarkerArk/MarkerArk/extractions.py\u001b[0m in \u001b[0;36mgetBiomarkerDiseaseRelations\u001b[0;34m(filename, session)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m#session = doc_parser.useCDRdata(session)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyXMLdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'BD Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/Documents/MallickLab/MarkerArk/MarkerArk/doc_parser.pyc\u001b[0m in \u001b[0;36mmyXMLdata\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpusParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxml_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mtrain_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BD Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/Documents/MallickLab/MarkerArk/MarkerArk/snorkel/snorkel/parser.pyc\u001b[0m in \u001b[0;36mparse_corpus\u001b[0;34m(self, session, name)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"session not none 2; committing corpus to session\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/Documents/MallickLab/MarkerArk/MarkerArk/snorkel/snorkel/models/context.pyc\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;34m\"\"\"Print summary / diagnostic stats about the corpus\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Number of documents:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_context_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchild_context_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/Documents/MallickLab/MarkerArk/MarkerArk/snorkel/snorkel/models/context.pyc\u001b[0m in \u001b[0;36mchild_context_stats\u001b[0;34m(self, parent_context)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m#print(query.statement)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Recurse to grandhildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/pandas/tools/plotting.pyc\u001b[0m in \u001b[0;36mhist_frame\u001b[0;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, **kwds)\u001b[0m\n\u001b[1;32m   2891\u001b[0m     fig, axes = _subplots(naxes=naxes, ax=ax, squeeze=False,\n\u001b[1;32m   2892\u001b[0m                           \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m                           layout=layout)\n\u001b[0m\u001b[1;32m   2894\u001b[0m     \u001b[0m_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/pandas/tools/plotting.pyc\u001b[0m in \u001b[0;36m_subplots\u001b[0;34m(naxes, sharex, sharey, squeeze, subplot_kw, ax, layout, layout_type, **fig_kw)\u001b[0m\n\u001b[1;32m   3378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3379\u001b[0m     \u001b[0;31m# Create first subplot separately, so we can share it if requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3380\u001b[0;31m     \u001b[0max0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/matplotlib/axes/_subplots.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     raise ValueError(\n\u001b[1;32m     63\u001b[0m                         \"num must be 1 <= num <= {maxn}, not {num}\".format(\n\u001b[0;32m---> 64\u001b[0;31m                             maxn=rows*cols, num=num))\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subplotspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# num - 1 for converting from MATLAB to python indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 0, not 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11555ab10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"articles/\"\n",
    "relations, sent, cand_class, session = extractions.getBiomarkerDiseaseRelations(filename, session)\n",
    "\n",
    "# dump relations just in case iPython breaks\n",
    "with open(\"articles.relations\", \"w+\") as f:\n",
    "     cPickle.dump(relations, f)\n",
    "#print relations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ae8fe04fdbf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'relations' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Arbitrary check of POS tagging on our Corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence(Document 3560095, 2, u'Improvements were seen in the number of tender joints, the severity of swelling and tenderness, the time of walk 50 feet, the duration of morning stiffness and the circumference of the left knee.')\n",
      "[u'Improvements', u'were', u'seen', u'in', u'the', u'number', u'of', u'tender', u'joints', u',', u'the', u'severity', u'of', u'swelling', u'and', u'tenderness', u',', u'the', u'time', u'of', u'walk', u'50', u'feet', u',', u'the', u'duration', u'of', u'morning', u'stiffness', u'and', u'the', u'circumference', u'of', u'the', u'left', u'knee', u'.']\n",
      "[u'NNP', u'VBD', u'VBN', u'IN', u'DT', u'NN', u'IN', u'NN', u'NNS', u',', u'DT', u'NN', u'IN', u'VBG', u'CC', u'NN', u',', u'DT', u'NN', u'IN', u'NN', u'CD', u'NNS', u',', u'DT', u'NN', u'IN', u'NN', u'NN', u'CC', u'DT', u'NN', u'IN', u'DT', u'JJ', u'NN', u'.']\n",
      "Sentence(Document 10526274, 7, u'Treatment was comprised of VNB, 25 mg/m(2), plus GEM, 1000 mg/m(2), both on Days 1, 8, and 15 every 28 days.')\n",
      "[u'Treatment', u'was', u'comprised', u'of', u'VNB', u',', u'25', u'mg/m', '(', u'2', ')', u',', u'plus', u'GEM', u',', u'1000', u'mg/m', '(', u'2', ')', u',', u'both', u'on', u'Days', u'1', u',', u'8', u',', u'and', u'15', u'every', u'28', u'days', u'.']\n",
      "[u'NN', u'VBD', u'VBN', u'IN', u'NNP', u',', u'CD', u'NN', u'-LRB-', u'CD', u'-RRB-', u',', u'CC', u'NN', u',', u'CD', u'NN', u'-LRB-', u'CD', u'-RRB-', u',', u'DT', u'IN', u'NNS', u'CD', u',', u'CD', u',', u'CC', u'CD', u'DT', u'CD', u'NNS', u'.']\n",
      "Sentence(Document 12443032, 0, u'Cocaine related chest pain: are we seeing the tip of an iceberg?')\n",
      "[u'Cocaine', u'related', u'chest', u'pain', u':', u'are', u'we', u'seeing', u'the', u'tip', u'of', u'an', u'iceberg', u'?']\n",
      "[u'NN', u'JJ', u'NN', u'NN', u':', u'VBP', u'PRP', u'VBG', u'DT', u'NN', u'IN', u'DT', u'NN', u'.']\n"
     ]
    }
   ],
   "source": [
    "# test = sent[0]\n",
    "# print test\n",
    "# print test.words\n",
    "# print test.pos_tags\n",
    "\n",
    "# test50 = sent[50]\n",
    "# print test50\n",
    "# print test50.words\n",
    "# print test50.pos_tags\n",
    "\n",
    "# test100 = sent[100]\n",
    "# print test100\n",
    "# print test100.words\n",
    "# print test100.pos_tags\n",
    "\n",
    "all_sents = []\n",
    "for s in sent:\n",
    "    all_sents.append(s)\n",
    "    \n",
    "test = all_sents[0]\n",
    "print test\n",
    "print test.words\n",
    "print test.pos_tags\n",
    "\n",
    "test50 = all_sents[50]\n",
    "print test50\n",
    "print test50.words\n",
    "print test50.pos_tags\n",
    "\n",
    "test100 = all_sents[100]\n",
    "print test100\n",
    "print test100.words\n",
    "print test100.pos_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3a) Internal Evaluation Labeling (Viewer)\n",
    "Manually generating labels with the viewer. We first define our subclass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"jupyter-js-widgets\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's get the candidates\n",
    "from snorkel.models import CandidateSet\n",
    "cs = session.query(CandidateSet).filter(CandidateSet.name == 'BD Development Candidates').one()\n",
    "cs\n",
    "\n",
    "# Launching the Viewer to inspect Candidates\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "sv = SentenceNgramViewer(cs[:300], session, annotator_name=\"BD Viewer User\")\n",
    "sv\n",
    "#print sv.get_selected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# 3b) External Evaluation Label Loading (Ground Truth/\"Gold\" Labels)\n",
    "\n",
    "Can be messy since external annotations can be in any format. Let's first load in the Labels from Mind Tagger (tsv format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e2_idxs', 'e1_label', 'e2_label', 'sent_id', 'e1_idxs', 'doc_id', 'is_correct']\n",
      "['[11]', 'MATCHER', 'MATCHER', '44', '[11]', 'pone', 'false']\n",
      "['[167, 168]', 'MATCHER', 'MATCHER', '4', '[51]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[7]', 'MATCHER', 'MATCHER', '21', '[20]', 'nihms389631', 'false']\n",
      "['[6, 7]', 'MATCHER', 'MATCHER', '51', '[14]', 'nihms389631', 'true']\n",
      "['[78, 79]', 'MATCHER', 'MATCHER', '0', '[71]', 'cam40003-1136', 'false']\n",
      "['[15, 16]', 'MATCHER', 'MATCHER', '6', '[29]', 'nihms143901', 'true']\n",
      "['[16]', 'MATCHER', 'MATCHER', '9', '[16]', 'pone', 'false']\n",
      "['[22]', 'MATCHER', 'MATCHER', '21', '[8]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[2]', 'MATCHER', 'MATCHER', '1', '[2]', 'cam40003-1136', 'false']\n",
      "['[48, 49]', 'MATCHER', 'MATCHER', '17', '[14]', 'pone', 'false']\n",
      "['[7, 8]', 'MATCHER', 'MATCHER', '2', '[75]', 'nihms356718', 'false']\n",
      "['[45]', 'MATCHER', 'MATCHER', '17', '[83]', 'pone', 'false']\n",
      "['[5, 6]', 'MATCHER', 'MATCHER', '9', '[14]', 'nihms143901', 'true']\n",
      "['[38]', 'MATCHER', 'MATCHER', '29', '[136]', 'pone', 'false']\n",
      "['[9, 10]', 'MATCHER', 'MATCHER', '36', '[9]', '5882', 'false']\n",
      "['[84, 85]', 'MATCHER', 'MATCHER', '39', '[55]', 'JO2012-709049', 'true']\n",
      "['[7]', 'MATCHER', 'MATCHER', '22', '[7]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[20, 21]', 'MATCHER', 'MATCHER', '9', '[33]', 'pone', 'UNKNOWN']\n",
      "['[33, 34, 35]', 'MATCHER', 'MATCHER', '6', '[10]', 'nihms604917', 'false']\n",
      "['[167, 168]', 'MATCHER', 'MATCHER', '4', '[55]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[10, 11]', 'MATCHER', 'MATCHER', '28', '[36]', 'zlj4536', 'false']\n",
      "['[36]', 'MATCHER', 'MATCHER', '23', '[33]', 'nihms389631', 'false']\n",
      "['[8, 9]', 'MATCHER', 'MATCHER', '9', '[31]', 'nihms551731', 'true']\n",
      "['[7]', 'MATCHER', 'MATCHER', '47', '[7]', 'nihms604917', 'false']\n",
      "['[61]', 'MATCHER', 'MATCHER', '0', '[77]', 'nihms389631', 'false']\n",
      "['[41]', 'MATCHER', 'MATCHER', '38', '[41]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[22]', 'MATCHER', 'MATCHER', '54', '[22]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[38]', 'MATCHER', 'MATCHER', '29', '[41]', 'pone', 'UNKNOWN']\n",
      "['[46, 47]', 'MATCHER', 'MATCHER', '2', '[47]', '5882', 'false']\n",
      "['[6]', 'MATCHER', 'MATCHER', '75', '[6]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[101]', 'MATCHER', 'MATCHER', '4', '[21]', 'cam40003-1136', 'UNKNOWN']\n",
      "['[22, 23]', 'MATCHER', 'MATCHER', '39', '[0]', 'JO2012-709049', 'true']\n",
      "['[11, 12]', 'MATCHER', 'MATCHER', '19', '[21]', '5882', 'UNKNOWN']\n",
      "['[30, 31]', 'MATCHER', 'MATCHER', '27', '[16]', '5882', 'UNKNOWN']\n",
      "['[73, 74]', 'MATCHER', 'MATCHER', '0', '[76]', 'cam40003-1136', 'false']\n",
      "['[58, 59]', 'MATCHER', 'MATCHER', '14', '[54]', 'pone', 'true']\n",
      "['[29, 30]', 'MATCHER', 'MATCHER', '32', '[14]', 'PNAS-1998-Belinsky-11891-6', 'false']\n",
      "['[39, 40]', 'MATCHER', 'MATCHER', '15', '[3]', 'JO2012-709049', 'true']\n",
      "['[44]', 'MATCHER', 'MATCHER', '13', '[30]', 'pone', 'false']\n",
      "['[184, 185]', 'MATCHER', 'MATCHER', '9', '[157]', 'cam40003-1136', 'false']\n",
      "['[73, 74]', 'MATCHER', 'MATCHER', '0', '[80]', 'cam40003-1136', 'false']\n",
      "['[20, 21]', 'MATCHER', 'MATCHER', '9', '[41]', 'pone', 'UNKNOWN']\n",
      "['[43, 44]', 'MATCHER', 'MATCHER', '38', '[5]', 'JO2012-709049', 'true']\n",
      "['[61]', 'MATCHER', 'MATCHER', '0', '[34]', 'nihms389631', 'false']\n",
      "['[49, 50]', 'MATCHER', 'MATCHER', '18', '[26]', 'JO2012-709049', 'true']\n",
      "['[20, 21]', 'MATCHER', 'MATCHER', '26', '[5]', 'nihms389631', 'false']\n",
      "['[8, 9]', 'MATCHER', 'MATCHER', '9', '[19]', 'nihms551731', 'true']\n",
      "['[5, 6]', 'MATCHER', 'MATCHER', '36', '[9]', '5882', 'false']\n",
      "['[7, 8, 9]', 'MATCHER', 'MATCHER', '75', '[0]', 'nihms604917', 'true']\n",
      "['[14, 15]', 'MATCHER', 'MATCHER', '9', '[60]', 'nihms551731', 'false']\n",
      "['[15, 16]', 'MATCHER', 'MATCHER', '7', '[43]', '5882', '\\\\N']\n",
      "['[16, 17]', 'MATCHER', 'MATCHER', '1', '[147]', 'zlj4536', '\\\\N']\n",
      "['[304, 305, 306]', 'MATCHER', 'MATCHER', '5', '[45]', 'nihms604917', '\\\\N']\n",
      "['[98]', 'MATCHER', 'MATCHER', '3', '[63]', 'cam40003-1136', '\\\\N']\n",
      "['[22, 23]', 'MATCHER', 'MATCHER', '32', '[58]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[44, 45]', 'MATCHER', 'MATCHER', '10', '[29]', 'nihms143901', '\\\\N']\n",
      "['[21, 22]', 'MATCHER', 'MATCHER', '0', '[75]', 'cam40003-1136', '\\\\N']\n",
      "['[25, 26]', 'MATCHER', 'MATCHER', '5', '[41]', '1-s2', '\\\\N']\n",
      "['[20, 21]', 'MATCHER', 'MATCHER', '19', '[13]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[1, 2]', 'MATCHER', 'MATCHER', '8', '[6]', 'cam40003-1136', '\\\\N']\n",
      "['[33, 34]', 'MATCHER', 'MATCHER', '45', '[16]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[19, 20]', 'MATCHER', 'MATCHER', '26', '[4]', 'nihms389631', '\\\\N']\n",
      "['[16, 17]', 'MATCHER', 'MATCHER', '2', '[6]', 'nihms356718', '\\\\N']\n",
      "['[16, 17]', 'MATCHER', 'MATCHER', '28', '[76]', 'zlj4536', '\\\\N']\n",
      "['[12, 13]', 'MATCHER', 'MATCHER', '18', '[4]', 'nihms389631', '\\\\N']\n",
      "['[36, 37]', 'MATCHER', 'MATCHER', '6', '[36]', '5882', '\\\\N']\n",
      "['[43, 44]', 'MATCHER', 'MATCHER', '9', '[40]', 'nihms356718', '\\\\N']\n",
      "['[136, 137]', 'MATCHER', 'MATCHER', '12', '[138]', '5882', '\\\\N']\n",
      "['[81]', 'MATCHER', 'MATCHER', '13', '[86]', 'pone', '\\\\N']\n",
      "['[24]', 'MATCHER', 'MATCHER', '59', '[10]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[15, 16]', 'MATCHER', 'MATCHER', '3', '[9]', 'JO2012-709049', '\\\\N']\n",
      "['[9, 10]', 'MATCHER', 'MATCHER', '17', '[3]', 'nihms389631', '\\\\N']\n",
      "['[191, 192]', 'MATCHER', 'MATCHER', '9', '[187]', 'cam40003-1136', '\\\\N']\n",
      "['[4, 5]', 'MATCHER', 'MATCHER', '70', '[19]', 'nihms356718', '\\\\N']\n",
      "['[31, 32]', 'MATCHER', 'MATCHER', '47', '[10]', 'pone', '\\\\N']\n",
      "['[304, 305, 306]', 'MATCHER', 'MATCHER', '5', '[39]', 'nihms604917', '\\\\N']\n",
      "['[17, 18]', 'MATCHER', 'MATCHER', '96', '[10]', 'nihms389631', '\\\\N']\n",
      "['[79]', 'MATCHER', 'MATCHER', '24', '[69]', 'nihms389631', '\\\\N']\n",
      "['[132, 133]', 'MATCHER', 'MATCHER', '0', '[13]', 'cam40003-1136', '\\\\N']\n",
      "['[26]', 'MATCHER', 'MATCHER', '6', '[0]', 'pone', '\\\\N']\n",
      "['[22, 23]', 'MATCHER', 'MATCHER', '24', '[29]', 'JO2012-709049', '\\\\N']\n",
      "['[18, 19]', 'MATCHER', 'MATCHER', '0', '[70]', 'cam40003-1136', '\\\\N']\n",
      "['[20]', 'MATCHER', 'MATCHER', '12', '[23]', 'pone', '\\\\N']\n",
      "['[77, 78]', 'MATCHER', 'MATCHER', '40', '[23]', '5882', '\\\\N']\n",
      "['[23, 24]', 'MATCHER', 'MATCHER', '18', '[8]', 'nihms551731', '\\\\N']\n",
      "['[5, 6]', 'MATCHER', 'MATCHER', '4', '[49]', 'nihms389631', '\\\\N']\n",
      "['[46, 47]', 'MATCHER', 'MATCHER', '2', '[50]', '5882', '\\\\N']\n",
      "['[27, 28]', 'MATCHER', 'MATCHER', '0', '[46]', 'cam40003-1136', '\\\\N']\n",
      "['[7]', 'MATCHER', 'MATCHER', '22', '[12]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[16, 17]', 'MATCHER', 'MATCHER', '8', '[30]', 'cam40003-1136', '\\\\N']\n",
      "['[29, 30]', 'MATCHER', 'MATCHER', '20', '[26]', 'pone', '\\\\N']\n",
      "['[6, 7]', 'MATCHER', 'MATCHER', '51', '[13]', '1-s2', '\\\\N']\n",
      "['[72, 73]', 'MATCHER', 'MATCHER', '3', '[66]', 'cam40003-1136', '\\\\N']\n",
      "['[13, 14]', 'MATCHER', 'MATCHER', '59', '[2]', '6246', '\\\\N']\n",
      "['[45]', 'MATCHER', 'MATCHER', '17', '[3]', 'pone', '\\\\N']\n",
      "['[12, 13]', 'MATCHER', 'MATCHER', '0', '[80]', 'cam40003-1136', '\\\\N']\n",
      "['[10]', 'MATCHER', 'MATCHER', '73', '[10]', 'nihms389631', '\\\\N']\n",
      "['[58, 59]', 'MATCHER', 'MATCHER', '14', '[39]', 'pone', '\\\\N']\n",
      "['[22, 23]', 'MATCHER', 'MATCHER', '32', '[47]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[26]', 'MATCHER', 'MATCHER', '48', '[26]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[10, 11]', 'MATCHER', 'MATCHER', '22', '[17]', 'nihms356718', '\\\\N']\n",
      "['[35, 36]', 'MATCHER', 'MATCHER', '2', '[81]', 'nihms356718', '\\\\N']\n",
      "['[10, 11]', 'MATCHER', 'MATCHER', '28', '[70]', 'zlj4536', '\\\\N']\n",
      "['[25]', 'MATCHER', 'MATCHER', '5', '[25]', 'nihms604917', '\\\\N']\n",
      "['[95, 96]', 'MATCHER', 'MATCHER', '8', '[75]', 'zlj4536', '\\\\N']\n",
      "['[19]', 'MATCHER', 'MATCHER', '20', '[19]', 'pone', '\\\\N']\n",
      "['[17]', 'MATCHER', 'MATCHER', '5', '[17]', 'nihms389631', '\\\\N']\n",
      "['[8]', 'MATCHER', 'MATCHER', '8', '[54]', 'pone', '\\\\N']\n",
      "['[15]', 'MATCHER', 'MATCHER', '6', '[5]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[66]', 'MATCHER', 'MATCHER', '3', '[99]', 'cam40003-1136', '\\\\N']\n",
      "['[15]', 'MATCHER', 'MATCHER', '58', '[15]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[84, 85]', 'MATCHER', 'MATCHER', '39', '[17]', 'JO2012-709049', '\\\\N']\n",
      "['[26]', 'MATCHER', 'MATCHER', '29', '[31]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[66, 67]', 'MATCHER', 'MATCHER', '25', '[25]', 'nihms551731', '\\\\N']\n",
      "['[25]', 'MATCHER', 'MATCHER', '7', '[6]', 'pone', '\\\\N']\n",
      "['[40, 41]', 'MATCHER', 'MATCHER', '14', '[16]', 'cam40003-1136', '\\\\N']\n",
      "['[31, 32]', 'MATCHER', 'MATCHER', '23', '[20]', 'zlj4536', '\\\\N']\n",
      "['[13, 14]', 'MATCHER', 'MATCHER', '8', '[0]', 'zlj4536', '\\\\N']\n",
      "['[4, 5, 6]', 'MATCHER', 'MATCHER', '32', '[14]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[33, 34]', 'MATCHER', 'MATCHER', '6', '[16]', 'JO2012-709049', '\\\\N']\n",
      "['[12, 13]', 'MATCHER', 'MATCHER', '2', '[5]', 'nihms-489269', '\\\\N']\n",
      "['[40, 41]', 'MATCHER', 'MATCHER', '8', '[22]', '5882', '\\\\N']\n",
      "['[3]', 'MATCHER', 'MATCHER', '2', '[3]', 'nihms389631', '\\\\N']\n",
      "['[16, 17]', 'MATCHER', 'MATCHER', '28', '[64]', 'zlj4536', '\\\\N']\n",
      "['[81]', 'MATCHER', 'MATCHER', '4', '[59]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[42, 43]', 'MATCHER', 'MATCHER', '33', '[34]', 'cam40003-1136', '\\\\N']\n",
      "['[9, 10]', 'MATCHER', 'MATCHER', '17', '[0]', 'nihms389631', '\\\\N']\n",
      "['[20, 21]', 'MATCHER', 'MATCHER', '20', '[4]', 'nihms143901', '\\\\N']\n",
      "['[25, 26]', 'MATCHER', 'MATCHER', '0', '[1]', '5882', '\\\\N']\n",
      "['[97]', 'MATCHER', 'MATCHER', '3', '[66]', 'cam40003-1136', '\\\\N']\n",
      "['[1, 2]', 'MATCHER', 'MATCHER', '14', '[31]', 'JO2012-709049', '\\\\N']\n",
      "['[9]', 'MATCHER', 'MATCHER', '11', '[14]', 'pone', '\\\\N']\n",
      "['[22]', 'MATCHER', 'MATCHER', '1', '[18]', 'cam40003-1136', '\\\\N']\n",
      "['[31, 32]', 'MATCHER', 'MATCHER', '47', '[17]', 'pone', '\\\\N']\n",
      "['[13, 14]', 'MATCHER', 'MATCHER', '80', '[4]', 'nihms604917', '\\\\N']\n",
      "['[8, 9]', 'MATCHER', 'MATCHER', '31', '[1]', 'cam40003-1136', '\\\\N']\n",
      "['[48, 49]', 'MATCHER', 'MATCHER', '55', '[89]', '5882', '\\\\N']\n",
      "['[22, 23]', 'MATCHER', 'MATCHER', '80', '[16]', 'JO2012-709049', '\\\\N']\n",
      "['[111, 112]', 'MATCHER', 'MATCHER', '1', '[106]', 'zlj4536', '\\\\N']\n",
      "['[33]', 'MATCHER', 'MATCHER', '3', '[65]', 'cam40003-1136', '\\\\N']\n",
      "['[11, 12]', 'MATCHER', 'MATCHER', '44', '[35]', '6246', '\\\\N']\n",
      "['[19]', 'MATCHER', 'MATCHER', '8', '[30]', 'cam40003-1136', '\\\\N']\n",
      "['[16, 17]', 'MATCHER', 'MATCHER', '2', '[25]', 'nihms356718', '\\\\N']\n",
      "['[67]', 'MATCHER', 'MATCHER', '30', '[56]', 'JO2012-709049', '\\\\N']\n",
      "['[18, 19]', 'MATCHER', 'MATCHER', '4', '[49]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[15, 16]', 'MATCHER', 'MATCHER', '6', '[36]', 'nihms143901', '\\\\N']\n",
      "['[26, 27]', 'MATCHER', 'MATCHER', '22', '[9]', '5882', '\\\\N']\n",
      "['[72, 73]', 'MATCHER', 'MATCHER', '3', '[96]', 'cam40003-1136', '\\\\N']\n",
      "['[29, 30]', 'MATCHER', 'MATCHER', '32', '[53]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[9]', 'MATCHER', 'MATCHER', '11', '[20]', 'pone', '\\\\N']\n",
      "['[12, 13]', 'MATCHER', 'MATCHER', '55', '[20]', 'pone', '\\\\N']\n",
      "['[304, 305, 306]', 'MATCHER', 'MATCHER', '5', '[155]', 'nihms604917', '\\\\N']\n",
      "['[6]', 'MATCHER', 'MATCHER', '84', '[9]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[16, 17]', 'MATCHER', 'MATCHER', '10', '[37]', 'nihms551731', '\\\\N']\n",
      "['[30, 31]', 'MATCHER', 'MATCHER', '31', '[1]', 'cam40003-1136', '\\\\N']\n",
      "['[22, 23]', 'MATCHER', 'MATCHER', '114', '[6]', 'nihms389631', '\\\\N']\n",
      "['[10, 11]', 'MATCHER', 'MATCHER', '19', '[5]', 'JO2012-709049', '\\\\N']\n",
      "['[43, 44]', 'MATCHER', 'MATCHER', '38', '[30]', 'JO2012-709049', '\\\\N']\n",
      "['[17, 18]', 'MATCHER', 'MATCHER', '33', '[10]', 'JO2012-709049', '\\\\N']\n",
      "['[9, 10]', 'MATCHER', 'MATCHER', '8', '[20]', 'cam40003-1136', '\\\\N']\n",
      "['[304, 305, 306]', 'MATCHER', 'MATCHER', '5', '[224]', 'nihms604917', '\\\\N']\n",
      "['[36]', 'MATCHER', 'MATCHER', '23', '[38]', 'nihms389631', '\\\\N']\n",
      "['[25, 26]', 'MATCHER', 'MATCHER', '11', '[16]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[304, 305, 306]', 'MATCHER', 'MATCHER', '5', '[261]', 'nihms604917', '\\\\N']\n",
      "['[19, 20, 21]', 'MATCHER', 'MATCHER', '50', '[23]', '1-s2', '\\\\N']\n",
      "['[20, 21]', 'MATCHER', 'MATCHER', '118', '[16]', 'nihms389631', '\\\\N']\n",
      "['[167, 168]', 'MATCHER', 'MATCHER', '4', '[35]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[21, 22]', 'MATCHER', 'MATCHER', '0', '[70]', 'cam40003-1136', '\\\\N']\n",
      "['[9, 10]', 'MATCHER', 'MATCHER', '8', '[54]', 'cam40003-1136', '\\\\N']\n",
      "['[8]', 'MATCHER', 'MATCHER', '21', '[8]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[32]', 'MATCHER', 'MATCHER', '3', '[67]', 'cam40003-1136', '\\\\N']\n",
      "['[12]', 'MATCHER', 'MATCHER', '22', '[12]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[30, 31]', 'MATCHER', 'MATCHER', '19', '[15]', 'pone', '\\\\N']\n",
      "['[82, 83]', 'MATCHER', 'MATCHER', '1', '[15]', 'cam40003-1136', '\\\\N']\n",
      "['[22]', 'MATCHER', 'MATCHER', '8', '[26]', 'cam40003-1136', '\\\\N']\n",
      "['[22]', 'MATCHER', 'MATCHER', '2', '[8]', 'cam40003-1136', '\\\\N']\n",
      "['[38]', 'MATCHER', 'MATCHER', '29', '[121]', 'pone', '\\\\N']\n",
      "['[81, 82]', 'MATCHER', 'MATCHER', '24', '[0]', 'cam40003-1136', '\\\\N']\n",
      "['[165, 166]', 'MATCHER', 'MATCHER', '13', '[61]', 'JO2012-709049', '\\\\N']\n",
      "['[19]', 'MATCHER', 'MATCHER', '68', '[19]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[61]', 'MATCHER', 'MATCHER', '0', '[7]', 'nihms389631', '\\\\N']\n",
      "['[26, 27]', 'MATCHER', 'MATCHER', '45', '[18]', 'JO2012-709049', '\\\\N']\n",
      "['[75, 76]', 'MATCHER', 'MATCHER', '11', '[64]', '5882', '\\\\N']\n",
      "['[9, 10]', 'MATCHER', 'MATCHER', '2', '[29]', 'nihms143901', '\\\\N']\n",
      "['[7]', 'MATCHER', 'MATCHER', '22', '[9]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[21, 22]', 'MATCHER', 'MATCHER', '31', '[15]', 'cam40003-1136', '\\\\N']\n",
      "['[21, 22]', 'MATCHER', 'MATCHER', '28', '[120]', 'zlj4536', '\\\\N']\n",
      "['[29]', 'MATCHER', 'MATCHER', '0', '[46]', 'cam40003-1136', '\\\\N']\n",
      "['[140, 141]', 'MATCHER', 'MATCHER', '29', '[110]', 'pone', '\\\\N']\n",
      "['[7, 8]', 'MATCHER', 'MATCHER', '15', '[10]', 'cam40003-1136', '\\\\N']\n",
      "['[3, 4]', 'MATCHER', 'MATCHER', '28', '[40]', 'nihms551731', '\\\\N']\n",
      "['[30]', 'MATCHER', 'MATCHER', '10', '[32]', 'pone', '\\\\N']\n",
      "['[30, 31]', 'MATCHER', 'MATCHER', '0', '[76]', 'cam40003-1136', '\\\\N']\n",
      "['[180]', 'MATCHER', 'MATCHER', '9', '[15]', 'cam40003-1136', '\\\\N']\n",
      "['[2]', 'MATCHER', 'MATCHER', '1', '[22]', 'cam40003-1136', '\\\\N']\n",
      "['[12, 13]', 'MATCHER', 'MATCHER', '5', '[25]', 'nihms-489269', '\\\\N']\n",
      "['[6, 7]', 'MATCHER', 'MATCHER', '27', '[31]', 'nihms551731', '\\\\N']\n",
      "['[189, 190, 191, 192]', 'MATCHER', 'MATCHER', '9', '[98]', 'cam40003-1136', '\\\\N']\n",
      "['[12]', 'MATCHER', 'MATCHER', '26', '[12]', 'PNAS-1998-Belinsky-11891-6', '\\\\N']\n",
      "['[1, 2]', 'MATCHER', 'MATCHER', '126', '[5]', '5882', '\\\\N']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "labels = []\n",
    "with open(\"gold.tsv\") as tsvFile:\n",
    "    tsvReader = csv.reader(tsvFile, delimiter=\"\\t\")\n",
    "    for line in tsvReader:\n",
    "        print line\n",
    "        labels.append(line)\n",
    "        \n",
    "formattedTuples = []\n",
    "for label in labels:\n",
    "    score = 0\n",
    "    if label[6] == \"true\":\n",
    "        score = 1\n",
    "    elif label[6] == \"false\":\n",
    "        score = -1\n",
    "    formattedTuples.append((label[5] + \"::\" + label[3] + \"::[\" + label[4] + \", \" + label[0] + \"]::['\" + label[1] + \"', '\" + label[2] + \"']\", score))\n",
    "\n",
    "evalIDs = []\n",
    "evalScores = []\n",
    "for ft in formattedTuples:\n",
    "    evalIDs.append(ft[0])\n",
    "    evalScores.append(ft[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gmachiraju/Documents/MallickLab/MarkerArk/MarkerArk/\n",
      "<Element root at 0x115330bd8>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.loaders import ExternalAnnotationsLoader\n",
    "loader = ExternalAnnotationsLoader(session, cand_class, 'BD Development Candidates -- Gold','BD Development Labels -- Gold', expand_candidate_set=True)\n",
    "\n",
    "#utility functions from tutorial\n",
    "def get_docs_xml(filepath, doc_path=\".//document\", id_path=\".//id/text()\"):\n",
    "    xml = et.fromstring(open(filepath, 'rb').read())\n",
    "    print xml\n",
    "    return dict(zip(xml.xpath(id_path), xml.xpath(doc_path)))\n",
    "\n",
    "def get_CID_unary_mentions(doc_xml, doc, type):\n",
    "    \"\"\"\n",
    "    Get a set of unary disease mentions in argument-dict format,\n",
    "    for ExternalAnnotationsLoader\n",
    "    \"\"\"\n",
    "    for mesh_id, ms in get_CD_mentions_by_MESHID(doc_xml, doc.sentences)[type].iteritems():\n",
    "        for m in ms:\n",
    "            yield {type.lower() : TemporarySpan(parent=m[0], char_start=m[1], char_end=m[2])}\n",
    "\n",
    "            \n",
    "import lxml.etree as et\n",
    "from snorkel.models import CandidateSet, split_stable_id\n",
    "from snorkel.candidates import TemporarySpan\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import bz2\n",
    "\n",
    "ROOT = os.getcwd() + \"/\"\n",
    "print ROOT\n",
    "\n",
    "def load_BioC_disease_labels(loader, file_name):\n",
    "    # Get all the annotated Pubtator documents as XML trees\n",
    "    doc_xmls = get_docs_xml(ROOT + file_name)\n",
    "    for doc_id, doc_xml in doc_xmls.iteritems():\n",
    "    \n",
    "        # Get the corresponding Document object\n",
    "        stable_id = \"%s::document:0:0\" % doc_id\n",
    "        print stable_id\n",
    "        doc       = session.query(Document).filter(Document.stable_id == stable_id).first()\n",
    "        if doc is not None:\n",
    "        \n",
    "            # Use custom script + loader to add\n",
    "            for d in get_CID_unary_mentions(doc_xml, doc, 'Disease'):\n",
    "                loader.add(d)\n",
    "\n",
    "load_BioC_disease_labels(loader, 'gold.xml')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from snorkel.models import Label\n",
    "\n",
    "# cs = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Development Candidates -- Gold').one()\n",
    "# print len(cs)\n",
    "# print session.query(Label).filter(Label.key == loader.annotation_key).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4) Training Model via Data Programming\n",
    "We train our statistical mosel to differentiate between TRUE and FALSE Disease mentions.\n",
    "This training will be achieved via data programming, enabling us to train a model using only a modest amount of hand-labeled data for validation and testing. We will not use any training labels provided with the skipped usage of training data to simulate a more realistic scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set up\n",
    "train = session.query(CandidateSet).filter(CandidateSet.name == 'BD Training Candidates').one()\n",
    "#dont have yet:\n",
    "#dev = session.query(CandidateSet).filter(CandidateSet.name == 'BD Development Candidates').one()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate Features \n",
    "We can do so automatically via Snorkel. Recall that the goal is to distinguish TRUE vs FALSE mentions of biomarker-Disease Relations. Hence, we embed our `BD Training Candidates` in a feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================              ] 63%\n",
      "\n",
      "Loading sparse Feature matrix...\n",
      "CPU times: user 1min 19s, sys: 1.21 s, total: 1min 21s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnotationKey (DDL_WORD_SEQ_[ALCAM])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import FeatureManager\n",
    "\n",
    "feature_manager = FeatureManager()\n",
    "\n",
    "# create a feature set\n",
    "%time F_train = feature_manager.create(session, train, 'Train Features')\n",
    "\n",
    "# stuff that we have with features...can load interns' features as well (via mindtagger)\n",
    "#%time F_train2 = feature_manager.load(session, train, 'Train Features')\n",
    "\n",
    "F_train\n",
    "F_train.get_candidate(0)\n",
    "F_train.get_key(0)\n",
    "\n",
    "#maybe write to another features file: articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Creating Labeling Functions\n",
    "Labeling functions are a core tool of data programming. These are heuristic functions that inform the search about the direction to a goal, and therefore aim to classify candidates correctly. Their outputs will be automatically combined and denoised to estimate the probabilities of training labels for the training data.\n",
    "\n",
    "*Note: We should be creating document-, sentence-, and mention-level labeleing functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#from snorkel.lf_terms import *\n",
    "from snorkel.lf_helpers import get_doc_candidate_spans\n",
    "from snorkel.lf_helpers import get_sent_candidate_spans\n",
    "from snorkel.lf_helpers import get_left_tokens, get_right_tokens\n",
    "\n",
    "#umls_dict              = load_umls_dictionary()\n",
    "#chemicals              = load_chemdner_dictionary()\n",
    "#abbrv2text, text2abbrv = load_specialist_abbreviations()\n",
    "\n",
    "keyWords = [\"associate\", \"express\", \"marker\", \"biomarker\", \"elevated\", \"decreased\",\n",
    "            \"correlation\", \"correlates\", \"found\", \"diagnose\", \"variant\", \"appear\",\n",
    "            \"connect\", \"relate\", \"exhibit\", \"indicate\", \"signify\", \"show\", \"demonstrate\",\n",
    "            \"reveal\", \"suggest\", \"evidence\", \"elevation\", \"indication\", \"diagnosis\",\n",
    "            \"variation\", \"modification\", \"suggestion\", \"link\", \"derivation\", \"denote\",\n",
    "            \"denotation\", \"demonstration\", \"magnification\", \"depression\", \"boost\", \"level\",\n",
    "            \"advance\", \"augmentation\", \"lessening\", \"enhancement\", \"expression\", \"buildup\",\n",
    "            \"diminishing\", \"diminishment\", \"reduction\", \"drop\", \"dwindling\", \"lowering\"]\n",
    "\n",
    "negationWords = [\"not\", \"nor\", \"neither\"]\n",
    "\n",
    "# Document-level LFs:\n",
    "#--------------------\n",
    "#def LF_undefined_abbreviation(c):\n",
    "#    '''Candidate is a known abbreviation, but no corresponding full name in document'''\n",
    "#    doc_spans = get_doc_candidate_spans(c)\n",
    "#    phrase = c[0].get_span().lower()\n",
    "#    mentions = set([s.get_span().lower() for s in doc_spans])\n",
    "#    if len(phrase) > 1 and phrase in abbrv2text and not set(abbrv2text[phrase].keys()).intersection(mentions):\n",
    "#        return -\n",
    "    \n",
    "# Sentence-level LFs:\n",
    "#---------------------\n",
    "def LF_contiguous_mentions(c):\n",
    "    '''Contiguous candidates are likely wrong'''\n",
    "    neighbor_spans = get_sent_candidate_spans(c)\n",
    "    start, end = c[0].get_word_start(), c[0].get_word_end()\n",
    "    for s in neighbor_spans:\n",
    "        if s.get_word_end() + 1 == start or s.get_word_start() - 1 == end:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "# Mention-level LFs:\n",
    "#-------------------\n",
    "def LF_tumors_growths(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return 1 if re.search(\"^(\\w* ){0,2}(['] )*(tumor|tumour|polyp|pilomatricoma|cyst|lipoma)$\", phrase) else 0\n",
    "\n",
    "def LF_cancer(c):\n",
    "    '''<TYPE> cancer'''\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return 1 if re.search(\"\\w* cancer\",phrase) else 0\n",
    "\n",
    "def LF_disease_syndrome(c):\n",
    "    '''<TYPE> disease or <TYPE> syndrome'''\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return 1 if re.search(\"\\w* (disease|syndrome)+\",phrase) else 0\n",
    "\n",
    "#def LF_indicators(c):\n",
    "#    '''Indicator words'''\n",
    "#    return 1 if \" \".join(c[0].get_attrib_tokens()).lower() in indicators else 0\n",
    "\n",
    "#def LF_common_disease(c):\n",
    "#    '''Common disease'''\n",
    "#    return 1 if \" \".join(c[0].get_attrib_tokens()).lower() in common_disease else 0\n",
    "\n",
    "#def LF_common_disease_acronyms(c):\n",
    "#    '''Common disease acronyms'''\n",
    "#    return 1 if \" \".join(c[0].get_attrib_tokens()) in common_disease_acronyms else 0\n",
    "\n",
    "def LF_deficiency_of(c):\n",
    "    '''deficiency of <TYPE>'''\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "    return 1 if phrase.endswith('deficiency') or phrase.startswith('deficiency') or phrase.endswith('dysfunction') else 0\n",
    "\n",
    "#def LF_positive_indicator(c):\n",
    "#    flag = False\n",
    "#    for i in c[0].get_attrib_tokens():\n",
    "#        if i.lower() in positive_indicator:\n",
    "#            flag = True\n",
    "#            break\n",
    "#    return 1 if flag else 0\n",
    "\n",
    "def LF_left_positive_argument(c):    \n",
    "    phrase = \" \".join(c[0].get_attrib_tokens('lemmas')).lower()\n",
    "    pattern = \"(\\w+ ){1,2}(infection|lesion|neoplasm|attack|defect|anomaly|abnormality|degeneration|carcinoma|lymphoma|tumor|tumour|deficiency|malignancy|hypoplasia|disorder|deafness|weakness|condition|dysfunction|dystrophy)$\"\n",
    "    return 1 if re.search(pattern,phrase) else 0\n",
    "\n",
    "def LF_right_negative_argument(c):    \n",
    "    phrase = \" \".join(c[0].get_attrib_tokens('lemmas')).lower()\n",
    "    pattern = \"^(history of|mitochondrial|amino acid)( \\w+){1,2}\"\n",
    "    return 1 if re.search(pattern, phrase) else 0\n",
    "\n",
    "def LF_medical_afixes(c):\n",
    "    pattern = \"(\\w+(pathy|stasis|trophy|plasia|itis|osis|oma|asis|asia)$|^(hyper|hypo)\\w+)\"\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens('lemmas')).lower()\n",
    "    return 1 if re.search(pattern,phrase) else 0\n",
    "\n",
    "#def LF_adj_diseases(c):\n",
    "#    return 1 if ' '.join(c[0].get_attrib_tokens()) in adj_diseases else 0\n",
    "\n",
    "\n",
    "# Dictionary LFs:\n",
    "#----------------\n",
    "#def LF_SNOWMED_CT_sign_or_symptom(c):\n",
    "#    return 1 if c[0].get_span() in umls_dict[\"snomedct\"][\"sign_or_symptom\"] else 0\n",
    "\n",
    "#def LF_SNOWMED_CT_disease_or_syndrome(c):\n",
    "#    return 1 if c[0].get_span() in umls_dict[\"snomedct\"][\"disease_or_syndrome\"] else 0\n",
    "\n",
    "#def LF_MESH_disease_or_syndrome(c):\n",
    "#    return 1 if c[0].get_span() in umls_dict[\"mesh\"][\"disease_or_syndrome\"] else 0\n",
    "\n",
    "#def LF_MESH_sign_or_symptom(c):\n",
    "#    return 1 if c[0].get_span() in umls_dict[\"mesh\"][\"sign_or_symptom\"] else 0\n",
    "\n",
    "\n",
    "# Negative LFs:\n",
    "#--------------\n",
    "#def LF_organs(c):\n",
    "#    phrase = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "#    return -1 if phrase in organs else 0      \n",
    "\n",
    "#def LF_chemical_name(c):\n",
    "#    phrase = \" \".join(c[0].get_attrib_tokens())\n",
    "#    return -1 if phrase in chemicals and not phrase.isupper() else 0\n",
    "\n",
    "#def LF_bodysym(c):\n",
    "#    phrase = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "#    return -1 if phrase in bodysym else 0  \n",
    "\n",
    "def LF_protein_chemical_abbrv(c):\n",
    "    '''Gene/protein/chemical name'''\n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"\\d+\",lemma) else 0\n",
    "\n",
    "def LF_base_pair_seq(c): \n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"^[GACT]{2,}$\",lemma) else 0\n",
    "\n",
    "#def LF_too_vague(c):\n",
    "#    phrase = \" \".join(c[0].get_attrib_tokens('lemmas')).lower()\n",
    "#    phrase_ = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "#    return -1 if phrase in vague or phrase_ in vague else 0\n",
    "\n",
    "def LF_neg_surfix(c):\n",
    "    terms = ['deficiency', 'the', 'the', 'of', 'to', 'a']\n",
    "    rw = get_right_tokens(c, window=1, attrib='lemmas')\n",
    "    if len(rw) > 0 and rw[0].lower() in terms:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "#def LF_non_common_disease(c):\n",
    "#    '''Non common diseases'''\n",
    "#    return -1 if \" \".join(c[0].get_attrib_tokens()).lower() in non_common_disease else 0\n",
    "\n",
    "#def LF_non_disease_acronyms(c):\n",
    "#    '''Non common disease acronyms'''\n",
    "#    return -1 if \" \".join(c[0].get_attrib_tokens()) in non_disease_acronyms else 0\n",
    "\n",
    "def LF_pos_in(c):\n",
    "    '''Candidates beginning with a preposition or subordinating conjunction'''\n",
    "    pos_tags = c[0].get_attrib_tokens('pos_tags')\n",
    "    return -1 if \"IN\" in pos_tags[0:1] else 0\n",
    "\n",
    "\n",
    "#def LF_right_window_incomplete(c):\n",
    "#    return -1 if right_terms.intersection(get_right_tokens(c, window=2, attrib='lemmas')) else 0\n",
    "\n",
    "#def LF_negative_indicator(c):\n",
    "#    flag = False\n",
    "#    for i in c[0].get_attrib_tokens():\n",
    "#        if i.lower() in negative_indicator:\n",
    "#            flag = True\n",
    "#            break\n",
    "#    return -1 if flag else 0\n",
    "\n",
    "x = '''\n",
    "# extra custom\n",
    "#--------------\n",
    "def presenceOfNot(m):\n",
    "    for word in negationWords:\n",
    "        if (word in m[0].get_right_tokens('lemmas', 20)) and (word in m.pre_window2('lemmas', 20)):\n",
    "            return True\n",
    "    return False\n",
    "# 1\n",
    "def LF_remove_same_word(m):\n",
    "    if(m.mention1(attribute='words')[0] == m.mention2(attribute='words')[0]):\n",
    "        return -1\n",
    "    \n",
    "def LF_distance(m):\n",
    "    print \"FIRST\"\n",
    "    print type(m)\n",
    "    # if 'neuroendocrine' in m.lemmas:\n",
    "    #     print m.lemmas\n",
    "    # print m.dep_labels\n",
    "    distance = abs(m.e2_idxs[0] - m.e1_idxs[0])\n",
    "    count = 0\n",
    "    for lemma in m.lemmas:\n",
    "        if lemma == ',':\n",
    "            count += 1\n",
    "    if count > 1 and ',' in m.pre_window1('lemmas', 1):\n",
    "        print m\n",
    "        return 0\n",
    "    if distance == 0:\n",
    "        return -1\n",
    "    if distance < 8:\n",
    "        # print \"RETURNING ONE\"\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def LF_roman_numeral(m):\n",
    "    biomarker = (m.mention1(attribute='words')[0])\n",
    "    unicodedata.normalize('NFKD', biomarker).encode('ascii','ignore')\n",
    "    if re.match(r'((?<=\\s)|(?<=^))(M{1,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})|M{0,4}(CM|CD|D?C{1,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})|M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{1,3})(IX|IV|V?I{0,3})|M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{1,3}))(?=\\s)',\n",
    "                biomarker):\n",
    "        print \"MATCHED ROMAN\"\n",
    "        print m.mention1(attribute='words')\n",
    "        return -1\n",
    "\n",
    "# 4\n",
    "def LF_marker(m):\n",
    "    return 1 if ( ('marker' in m[0].get_attrib_tokens('lemmas', 6) or 'biomarker' in m[.post_window1('lemmas', 6)) and (\n",
    "        'marker' in m.pre_window2('lemmas', 6) or 'biomarker' in m.pre_window2('lemmas', 6)) ) or (('marker' in m.pre_window1('lemmas', 6) or 'biomarker' in m.pre_window1('lemmas', 6)) and (\n",
    "        'marker' in m.post_window2('lemmas', 6) or 'biomarker' in m.post_window2('lemmas', 6)))  else 0\n",
    "\n",
    "# 9 (-1 if biomarker is confused with a name of a person)\n",
    "def LF_People(m):\n",
    "    return -1 if ('NNP' in m.mention1(attribute='poses')) else 0\n",
    "# 51\n",
    "def LF_possible(m):\n",
    "    return -1 if ('possible' in m.pre_window1('lemmas', 20)) else 0\n",
    "# 52\n",
    "def LF_explore(m):\n",
    "    return -1 if ('explore' in m.pre_window1('lemmas', 20)) else 0\n",
    "# 53\n",
    "def LF_key(m):\n",
    "    # print m.pre_window1('lemmas', 20)\n",
    "    return -1 if ('abbreviation' in m.pre_window1('lemmas', 20) or (\n",
    "        'word' in m.pre_window1('lemmas', 20) and 'key' in m.pre_window1('lemmas', 20))) else 0\n",
    "# 54\n",
    "def LF_investigate(m):\n",
    "    return -1 if ('investigate' in m.pre_window1('lemmas', 20)) else 0\n",
    "# 55\n",
    "def LF_yetToBeConfirmed(m):\n",
    "    return -1 if ('yet' in m.post_window1('lemmas', 20) and 'to' in m.post_window1('lemmas', 20) and 'be' in m.post_window1(\n",
    "        'lemmas', 20) and 'confirmed' in m.post_window1('lemmas', 20)) else 0\n",
    "# 56\n",
    "def LF_notAssociated(m):\n",
    "    return -1 if ('not' in m.post_window1('lemmas', 20) and 'associated' in m.post_window2('lemmas', 20)) else 0\n",
    "# 56\n",
    "def LF_notRelated(m):\n",
    "    return -1 if ('not' in m.post_window1('lemmas', 20) and 'related' in m.post_window2('lemmas', 20)) else 0\n",
    "# 57\n",
    "def LF_doesNotShow(m):\n",
    "    return -1 if (\n",
    "        'does' in m.post_window1('lemmas', 20) and 'not' in m.post_window1('lemmas', 20) and 'show' in m.post_window2(\n",
    "            'lemmas', 20)) else 0\n",
    "# 58\n",
    "def LF_notLinked(m):\n",
    "    return -1 if ('not' in m.post_window1('lemmas', 20) and 'linked' in m.post_window2('lemmas', 20)) else 0\n",
    "# 59\n",
    "def LF_notCorrelated(m):\n",
    "    return -1 if ('not' in m.post_window1('lemmas', 20) and 'correlated' in m.post_window2('lemmas', 20)) else 0\n",
    "# 60\n",
    "def LF_disprove(m):\n",
    "    return -1 if ('disprove' in m.post_window1('lemmas', 20)) else 0\n",
    "# 62\n",
    "def LF_doesNotSignify(m):\n",
    "    return -1 if (\n",
    "        'does' in m.post_window1('lemmas', 20) and 'not' in m.post_window1('lemmas', 20) and 'signify' in m.post_window(\n",
    "            'lemmas', 20)) else 0\n",
    "# 63\n",
    "def LF_doesNotIndicate(m):\n",
    "    print \"SECOND\"\n",
    "    return -1 if (\n",
    "        'does' in m.post_window1('lemmas', 20) and 'not' in m.post_window1('lemmas', 20) and 'indicate' in m.post_window(\n",
    "            'lemmas', 20)) else 0\n",
    "# 64\n",
    "def LF_doesNotImply(m):\n",
    "    print \"THIRD\"\n",
    "    return -1 if (\n",
    "        'does' in m.post_window1('lemmas', 20) and 'not' in m.post_window1('lemmas', 20) and 'imply' in m.post_window(\n",
    "            'lemmas', 20)) else 0\n",
    "# 65\n",
    "def LF_studies(m):\n",
    "    return 1 if (\n",
    "        'studies' in m.pre_window1('lemmas', 20) and 'have' in m.pre_window1('lemmas', 20) and'reported' in m.pre_window1(\n",
    "            'lemmas', 20)) else 0\n",
    "# 66\n",
    "def LF_studies2(m):\n",
    "    return 1 if (\n",
    "        'studies' in m.pre_window1('lemmas', 20) and 'have' in m.pre_window1('lemmas', 20) and 'disclosed' in m.pre_window1(\n",
    "            'lemmas', 20)) else 0\n",
    "# 67\n",
    "def LF_studies3(m):\n",
    "    return 1 if (\n",
    "        'studies' in m.pre_window1('lemmas', 20) and 'have' in m.pre_window1('lemmas', 20) and'disclosed' in m.pre_window1('lemmas', 20)) else 0\n",
    "# 68\n",
    "def LF_studies4(m):\n",
    "    return 1 if (\n",
    "        'studies' in m.pre_window1('lemmas', 20) and 'have' in m.pre_window1('lemmas', 20) and 'expressed' in m.pre_window1(\n",
    "            'lemmas', 20)) else 0\n",
    "# 69\n",
    "def LF_interesting(m):\n",
    "    return 1 if (\n",
    "        'is' in m.post_window1('lemmas', 20) and 'an' in m.post_window1('lemmas', 20) and 'interesting' in m.post_window1(\n",
    "            'lemmas', 20) and 'target' in m.post_window1('lemmas', 20) and 'is' in m.pre_window2('lemmas', 20) and 'an' in\n",
    "        m.pre_window2('lemmas', 20) and 'interesting' in m.pre_window2('lemmas', 20) and 'target' in m.pre_window2(\n",
    "            'lemmas', 20)) else 0\n",
    "# 70\n",
    "def LF_discussion(m):\n",
    "    return 1 if (\n",
    "        'discussion' in m.pre_window1('lemmas', 20)) else 0\n",
    "# 71\n",
    "def LF_conclusion(m):\n",
    "    if ('conclusion' in m.pre_window1('lemmas', 20) or 'conclusion' in m.pre_window2('lemmas', 20)):\n",
    "        # print \"FOUND\"\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# 72\n",
    "def LF_recently(m):\n",
    "    return 1 if (\n",
    "        'recently' in m.pre_window1('lemmas', 20) or 'recently' in m.post_window1('lemmas', 20)) else 0\n",
    "# 73\n",
    "def LF_induced(m):\n",
    "    return 1 if (\n",
    "        'induced' in m.post_window1('lemmas', 20) and 'induced' in m.pre_window2('lemmas', 20)) else 0\n",
    "# 74\n",
    "def LF_treatment(m):\n",
    "    return 1 if (\n",
    "        'treatment' in m.pre_window1('lemmas', 20) or 'treatment' in m.post_window1('lemmas', 20)) else 0\n",
    "# 75\n",
    "def LF_auxpass(m):\n",
    "    if not ('auxpass' and 'aux') in (m.post_window1('dep_labels', 20) and m.pre_window2('dep_labels', 20)):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "# 75\n",
    "def LF_inbetween(m):\n",
    "    # with open('diseaseDatabase.pickle', 'rb') as f:\n",
    "    #     diseaseDictionary = pickle.load(f)\n",
    "    # with open('diseaseAbbreviationsDatabase.pickle', 'rb') as f:\n",
    "    #     diseaseAbb = pickle.load(f)\n",
    "    # with open('markerData.pickle', 'rb') as f:\n",
    "    #     markerDatabase = pickle.load(f)\n",
    "    # for marker in markerDatabase:\n",
    "    #     if(marker in list[m.e1_idxs[0] : m.e2_idxs[0]]):\n",
    "    #         return -1\n",
    "    # for disease in diseaseDictionary:\n",
    "    #     if (disease in list[m.e1_idxs[0]: m.e2_idxs[0]]):\n",
    "    #         return -1\n",
    "    # for disease in diseaseAbb:\n",
    "    #     if (marker in list[m.e1_idxs[0]: m.e2_idxs[0]]):\n",
    "    #         return -1\n",
    "    return 0\n",
    "# 76\n",
    "def LF_patientsWith(m):\n",
    "    return 1 if ('patient' in m.post_window2('lemmas', 3)) and ('with' in m.post_window2('lemmas',2)) else 0\n",
    "# 77\n",
    "def LF_isaBiomarker(m):\n",
    "    post_window1_lemmas = m.post_window1('lemmas',20)\n",
    "    pre_window2_lemmas = m.pre_window2('lemmas',20)\n",
    "    if ('biomarker' in post_window1_lemmas and 'biomarker' in pre_window2_lemmas) or ('marker' in post_window1_lemmas and 'marker' in pre_window2_lemmas) or ('indicator' in post_window1_lemmas and 'indicator' in pre_window2_lemmas):\n",
    "        marker_idx_post_window1 = -1\n",
    "        markers = ['biomarker','marker','indicator']\n",
    "        for marker in markers:\n",
    "            try:\n",
    "                # print post_window1_lemmas\n",
    "                findMarker = post_window1_lemmas.index(marker)\n",
    "                if not findMarker == -1:\n",
    "                    marker_idx_post_window1 = findMarker\n",
    "                    print marker\n",
    "            except:\n",
    "                pass\n",
    "        if 'cop' in m.post_window1('dep_labels',20):\n",
    "            try:\n",
    "                cop_idx_post_window1 = m.post_window1('dep_labels',20).index('cop')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print \"MarkerIdx:\"\n",
    "            print marker_idx_post_window1\n",
    "            print \"ROOTIdx:\"\n",
    "            try:\n",
    "                print  m.post_window1('dep_labels',marker_idx_post_window1)\n",
    "                print  m.post_window1('dep_labels',marker_idx_post_window1).index('ROOT')\n",
    "            except:\n",
    "                pass\n",
    "            print '\\n'\n",
    "            \n",
    "            return 1 if ('nsubj' in m.mention1(attribute='dep_labels')) and (marker_idx_post_window1-cop_idx_post_window1 < 4)  else 0\n",
    "    return 0\n",
    "# 78\n",
    "def LF_suspect(m):\n",
    "    return -1 if ('suspect' in m.pre_window1('lemmas', 20) or 'suspect' in m.post_window1('lemmas', 20)) else 0\n",
    "# 79\n",
    "def LF_mark(m):\n",
    "    return -1 if ( 'vmod' in m.post_window1('dep_labels', 20) and 'mark' in m.post_window1('dep_labels', 20) or'vmod' in m.pre_window1('dep_labels', 20) and 'mark' in m.pre_window1('dep_labels', 20)) else 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Apply the Labeling Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================              ] 63%\n",
      "\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 16.8 s, sys: 860 ms, total: 17.7 s\n",
      "Wall time: 17 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conflicts</th>\n",
       "      <th>coverage</th>\n",
       "      <th>j</th>\n",
       "      <th>overlaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_contiguous_mentions</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048394</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_tumors_growths</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_cancer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_disease_syndrome</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_deficiency_of</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_left_positive_argument</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_right_negative_argument</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_medical_afixes</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_protein_chemical_abbrv</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193577</td>\n",
       "      <td>8</td>\n",
       "      <td>0.010559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_base_pair_seq</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_surfix</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_pos_in</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            conflicts  coverage   j  overlaps\n",
       "LF_contiguous_mentions            0.0  0.048394   0  0.012758\n",
       "LF_tumors_growths                 0.0  0.000000   1  0.000000\n",
       "LF_cancer                         0.0  0.000000   2  0.000000\n",
       "LF_disease_syndrome               0.0  0.000000   3  0.000000\n",
       "LF_deficiency_of                  0.0  0.000000   4  0.000000\n",
       "LF_left_positive_argument         0.0  0.000000   5  0.000000\n",
       "LF_right_negative_argument        0.0  0.000000   6  0.000000\n",
       "LF_medical_afixes                 0.0  0.002640   7  0.000000\n",
       "LF_protein_chemical_abbrv         0.0  0.193577   8  0.010559\n",
       "LF_base_pair_seq                  0.0  0.001320   9  0.000440\n",
       "LF_neg_surfix                     0.0  0.010999  10  0.002640\n",
       "LF_pos_in                         0.0  0.001760  11  0.001760"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LFs = LFs_mention + LFs_dicts + LFs_false\n",
    "\n",
    "#LFs = [LF_investigate, LF_key, LF_distance, LF_auxpass, LF_inbetween,\n",
    "#       LF_possible, LF_explore, LF_key, LF_investigate, LF_yetToBeConfirmed, LF_notAssociated, LF_notRelated,\n",
    "#       LF_doesNotShow, LF_notLinked, LF_notCorrelated, LF_disprove, LF_doesNotSignify,\n",
    "#       LF_doesNotIndicate, LF_doesNotImply, LF_studies, LF_studies2, LF_studies3, LF_studies4, LF_interesting,\n",
    "#       LF_discussion, LF_conclusion, LF_recently, LF_induced, LF_treatment, LF_isaBiomarker, LF_marker, LF_suspect, LF_mark, LF_People]\n",
    "\n",
    "LFs = [LF_contiguous_mentions, LF_tumors_growths, LF_cancer, LF_disease_syndrome, LF_deficiency_of, LF_left_positive_argument, LF_right_negative_argument, LF_medical_afixes, LF_protein_chemical_abbrv, LF_base_pair_seq, LF_neg_surfix, LF_pos_in]\n",
    "\n",
    "# First, we construct a `CandidateLabeler`.\n",
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "# Next we run the `CandidateLabeler` to to apply the labeling functions to the training `CandidateSet`. \n",
    "# We'll start with some of our labeling functions:\n",
    "%time L_train = label_manager.create(session, train, 'LF Labels', f=LFs)\n",
    "L_train\n",
    "\n",
    "# statistical summary:\n",
    "L_train.lf_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fitting the Generative Model\n",
    "We estimate the accuracies of the labeling functions without supervision. Specifically, we estimate the parameters of a NaiveBayes generative model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t2273\n",
      "Features:\t\t\t12\n",
      "================================================================================\n",
      "Begin training for rate=1e-05, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 0.049023\n",
      "\tLearning epoch = 250\tGradient mag. = 0.051594\n",
      "\tLearning epoch = 500\tGradient mag. = 0.051584\n",
      "\tLearning epoch = 750\tGradient mag. = 0.051575\n",
      "\tLearning epoch = 1000\tGradient mag. = 0.051565\n",
      "\tLearning epoch = 1250\tGradient mag. = 0.051555\n",
      "\tLearning epoch = 1500\tGradient mag. = 0.051546\n",
      "\tLearning epoch = 1750\tGradient mag. = 0.051536\n",
      "\tLearning epoch = 2000\tGradient mag. = 0.051527\n",
      "\tLearning epoch = 2250\tGradient mag. = 0.051517\n",
      "\tLearning epoch = 2500\tGradient mag. = 0.051507\n",
      "\tLearning epoch = 2750\tGradient mag. = 0.051498\n",
      "Final gradient magnitude for rate=1e-05, mu=1e-06: 0.051\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "gen_model.train(L_train, n_iter=3000, rate=1e-5)\n",
    "gen_model.save(session, 'Generative Params')\n",
    "\n",
    "# We now apply the generative model to the training candidates.\n",
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training the Discriminative Model\n",
    "We use the estimated probabilites to train a discriminative model that classifies each `Candidate` as a true or false mention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t556\n",
      "Features:\t\t\t32969\n",
      "================================================================================\n",
      "Using gradient descent...\n",
      "\tLearning epoch = 0\tStep size = 0.001\n",
      "\tLoss = 385.389832\tGradient magnitude = 258.817501\n",
      "\tLearning epoch = 100\tStep size = 0.000904792147114\n",
      "\tLoss = 317.875314\tGradient magnitude = 3.400161\n",
      "\tLearning epoch = 200\tStep size = 0.000818648829479\n",
      "\tLoss = 317.389264\tGradient magnitude = 1.613463\n",
      "\tLearning epoch = 300\tStep size = 0.000740707032156\n",
      "\tLoss = 317.271242\tGradient magnitude = 0.973397\n",
      "\tLearning epoch = 400\tStep size = 0.000670185906007\n",
      "\tLoss = 317.229310\tGradient magnitude = 0.676232\n",
      "\tLearning epoch = 500\tStep size = 0.000606378944861\n",
      "\tLoss = 317.210742\tGradient magnitude = 0.516597\n",
      "\tLearning epoch = 600\tStep size = 0.000548646907485\n",
      "\tLoss = 317.201289\tGradient magnitude = 0.423235\n",
      "\tLearning epoch = 700\tStep size = 0.000496411413431\n",
      "\tLoss = 317.196029\tGradient magnitude = 0.366498\n",
      "\tLearning epoch = 800\tStep size = 0.00044914914861\n",
      "\tLoss = 317.192919\tGradient magnitude = 0.331848\n",
      "\tLearning epoch = 900\tStep size = 0.000406386622545\n",
      "\tLoss = 317.191025\tGradient magnitude = 0.311578\n",
      "\tLearning epoch = 1000\tStep size = 0.000367695424771\n",
      "\tLoss = 317.189874\tGradient magnitude = 0.301194\n",
      "\tLearning epoch = 1100\tStep size = 0.000332687932862\n",
      "\tLoss = 317.189213\tGradient magnitude = 0.297998\n",
      "\tLearning epoch = 1200\tStep size = 0.000301013429093\n",
      "\tLoss = 317.188892\tGradient magnitude = 0.299924\n",
      "\tLearning epoch = 1300\tStep size = 0.000272354586819\n",
      "\tLoss = 317.188831\tGradient magnitude = 0.306205\n",
      "\tLearning epoch = 1400\tStep size = 0.000246424291385\n",
      "\tLoss = 317.188978\tGradient magnitude = 0.315940\n",
      "\tLearning epoch = 1500\tStep size = 0.000222962763703\n",
      "\tLoss = 317.189297\tGradient magnitude = 0.328463\n",
      "\tLearning epoch = 1600\tStep size = 0.000201734957697\n",
      "\tLoss = 317.189765\tGradient magnitude = 0.343317\n",
      "\tLearning epoch = 1700\tStep size = 0.000182528205523\n",
      "\tLoss = 317.190369\tGradient magnitude = 0.360140\n",
      "\tLearning epoch = 1800\tStep size = 0.000165150086984\n",
      "\tLoss = 317.191104\tGradient magnitude = 0.378740\n",
      "\tLearning epoch = 1900\tStep size = 0.000149426501798\n",
      "\tLoss = 317.191965\tGradient magnitude = 0.398879\n",
      "\tLearning epoch = 2000\tStep size = 0.000135199925397\n",
      "\tLoss = 317.192952\tGradient magnitude = 0.420396\n",
      "\tLearning epoch = 2100\tStep size = 0.00012232783079\n",
      "\tLoss = 317.194060\tGradient magnitude = 0.443062\n",
      "\tLearning epoch = 2200\tStep size = 0.000110681260672\n",
      "\tLoss = 317.195287\tGradient magnitude = 0.467043\n",
      "\tLearning epoch = 2300\tStep size = 0.000100143535489\n",
      "\tLoss = 317.196652\tGradient magnitude = 0.492536\n",
      "\tLearning epoch = 2400\tStep size = 9.06090844946e-05\n",
      "\tLoss = 317.198148\tGradient magnitude = 0.519215\n",
      "\tLearning epoch = 2500\tStep size = 8.19823881078e-05\n",
      "\tLoss = 317.199792\tGradient magnitude = 0.547435\n",
      "\tLearning epoch = 2600\tStep size = 7.41770209616e-05\n",
      "\tLoss = 317.201548\tGradient magnitude = 0.576705\n",
      "\tLearning epoch = 2700\tStep size = 6.71147860624e-05\n",
      "\tLoss = 317.203441\tGradient magnitude = 0.606872\n",
      "\tLearning epoch = 2800\tStep size = 6.07249313844e-05\n",
      "\tLoss = 317.205515\tGradient magnitude = 0.638735\n",
      "\tLearning epoch = 2900\tStep size = 5.49434410507e-05\n",
      "\tLoss = 317.207743\tGradient magnitude = 0.671964\n",
      "\tLearning epoch = 3000\tStep size = 4.9712393998e-05\n",
      "\tLoss = 317.210123\tGradient magnitude = 0.706451\n",
      "\tLearning epoch = 3100\tStep size = 4.49793837036e-05\n",
      "\tLoss = 317.212680\tGradient magnitude = 0.742445\n",
      "\tLearning epoch = 3200\tStep size = 4.06969931571e-05\n",
      "\tLoss = 317.215407\tGradient magnitude = 0.779455\n",
      "\tLearning epoch = 3300\tStep size = 3.68223198197e-05\n",
      "\tLoss = 317.218325\tGradient magnitude = 0.818188\n",
      "\tLearning epoch = 3400\tStep size = 3.33165458113e-05\n",
      "\tLoss = 317.221425\tGradient magnitude = 0.857666\n",
      "\tLearning epoch = 3500\tStep size = 3.01445490191e-05\n",
      "\tLoss = 317.224755\tGradient magnitude = 0.899030\n",
      "\tLearning epoch = 3600\tStep size = 2.72745512307e-05\n",
      "\tLoss = 317.228303\tGradient magnitude = 0.942133\n",
      "\tLearning epoch = 3700\tStep size = 2.46777997696e-05\n",
      "\tLoss = 317.232025\tGradient magnitude = 0.986192\n",
      "\tLearning epoch = 3800\tStep size = 2.23282794396e-05\n",
      "\tLoss = 317.235877\tGradient magnitude = 1.031329\n",
      "\tLearning epoch = 3900\tStep size = 2.02024518955e-05\n",
      "\tLoss = 317.239903\tGradient magnitude = 1.077697\n",
      "\tLearning epoch = 4000\tStep size = 1.82790198275e-05\n",
      "\tLoss = 317.244066\tGradient magnitude = 1.125008\n",
      "\tLearning epoch = 4100\tStep size = 1.65387135968e-05\n",
      "\tLoss = 317.248353\tGradient magnitude = 1.173231\n",
      "\tLearning epoch = 4200\tStep size = 1.49640981858e-05\n",
      "\tLoss = 317.252821\tGradient magnitude = 1.223220\n",
      "\tLearning epoch = 4300\tStep size = 1.35393985271e-05\n",
      "\tLoss = 317.257483\tGradient magnitude = 1.274878\n",
      "\tLearning epoch = 4400\tStep size = 1.2250341464e-05\n",
      "\tLoss = 317.262323\tGradient magnitude = 1.328861\n",
      "\tLearning epoch = 4500\tStep size = 1.10840127561e-05\n",
      "\tLoss = 317.267322\tGradient magnitude = 1.384795\n",
      "\tLearning epoch = 4600\tStep size = 1.00287277002e-05\n",
      "\tLoss = 317.272480\tGradient magnitude = 1.441901\n",
      "\tLearning epoch = 4700\tStep size = 9.0739140687e-06\n",
      "\tLoss = 317.277693\tGradient magnitude = 1.498532\n",
      "\tLearning epoch = 4800\tStep size = 8.21000619294e-06\n",
      "\tLoss = 317.283097\tGradient magnitude = 1.558296\n",
      "\tLearning epoch = 4900\tStep size = 7.42834913113e-06\n",
      "\tLoss = 317.288703\tGradient magnitude = 1.620361\n",
      "CPU times: user 6.68 s, sys: 145 ms, total: 6.83 s\n",
      "Wall time: 6.84 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import LogReg\n",
    "\n",
    "disc_model = LogReg()\n",
    "disc_model.train(F_train, train_marginals, n_iter=5000, rate=1e-3)\n",
    "disc_model.w.shape\n",
    "%time disc_model.save(session, \"Discriminative Params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
